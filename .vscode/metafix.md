1. Причина проблемы и краткое решение CSC

Причина: В реализации JuiceFS с клиентом Rueidis была включена серверная функция отслеживания ключей Redis (client tracking) в режиме широковещательной рассылки (BCAST) и задан TTL кэша. Однако реальные операции чтения выполнялись через адаптер совместимости rueidiscompat (полученный из go-redis API) без использования методов кэширования (DoCache или compat.Cache). Из-за этого данные не помещались в локальный кэш Rueidis, и механизм клиентского кэширования не работал. В результате при большом TTL (например, 1 час) запросы к метаданным обслуживались быстро из локальной памяти, но новые файлы «не видны» другим клиентам (или даже тому же клиенту) до истечения TTL. При маленьком TTL обновления обнаруживаются быстрее, но кэш почти не используется и падает производительность.

Решение (CSC-модель): Необходимо полноценно задействовать кэширование на стороне клиента (CSC) с управляемой сервером инвалидацией. То есть все операции чтения метаданных должны выполняться с помощью Rueidis DoCache / DoMultiCache (или через rueidiscompat.Cache(TTL)), чтобы результаты кешировались локально на заданный TTL. Redis-сервер при этом будет отслеживать прочитанные ключи и автоматически рассылать уведомления об их изменении, сбрасывая устаревшие данные из кэша перед следующим запросом
github.com
github.com
. Таким образом достигается консистентность (новосозданный файл сразу становится виден всем клиентам) без жертвования быстродействием: метаданные читаются из локального «репликоподобного» кэша, который обновляется мгновенно при любом изменении
github.com
. Локальную Redis-реплику больше не понадобится содержать.

2. Пошаговый план реализации

Шаг 2.1: Реализация оберток для кэшированного чтения. В файле rueidis.go создадим вспомогательные функции (или методы) для чтения с кэшированием: например, cachedGet(ctx, key) []byte для получения значения по ключу и cachedHGet(ctx, key, field) []byte для получения поля хеша. Эти функции внутри будут вызывать Rueidis с пометкой на кэширование. Например, с использованием адаптера rueidiscompat это будет выглядеть так: m.compat.Cache(ttl).Get(ctx, key).Result() и m.compat.Cache(ttl).HGet(ctx, hashKey, field).Result(). Метод Cache(ttl) в адаптере Rueidis включит CSC для следующей команды с заданным TTL. TTL следует брать из конфигурации (тот же, что использовался ранее для client tracking, например MetaCacheTTL ~ 1 час, либо настраиваемый). При реализации учитываем, что Result() для HGet может вернуть ошибку rueidiscompat.Nil – её интерпретируем как отсутствие ключа/поля (ENOENT).

Шаг 2.2: Замена обычных чтений на кэшируемые. Проходим по всем операциям чтения метаданных в rueidis.go и связанных файлах, заменяя обращения через m.compat на использование новых кэш-функций:

Получение атрибутов inode (GetAttr) – вместо прямого m.compat.Get используем cachedGet. Например, в реализации rueidisMeta.GetAttr (или внутренней doGetAttr) нужно вызвать cachedGet(ctx, inodeKey) для чтения атрибутов inode из Redis с кэшированием. Возвращённые байты декодируем в структуру Attr как раньше. Теперь при первом обращении атрибут inode кешируется на TTL, и последующие getattr будут мгновенно из памяти до истечения TTL или до инвалидации при изменении.

Lookup (поиск имени файла) – модернизируем rueidisMeta.doLookup. Ранее там мог использоваться скрипт Lua (m.shaLookup) для атомарного получения inode и атрибутов. В условиях CSC от этого скрипта можно отказаться, поскольку кэширование сглаживает задержки. Реализуем логику так: сначала через cachedHGet читаем в родительском каталоге ключ entry:{parent} поле с именем искомого файла. Если поле не найдено – возвращаем ENOENT (как и раньше). Если получили inode нового файла, то при необходимости атрибуты получаем отдельным вызовом cachedGet по ключу inode. Оба этих чтения будут отслеживаться сервером. В случае создания нового файла: при выполнении HSET entry:{parent} и SET inode:{id} Redis разошлёт уведомления, и у всех клиентов (включая создавший) кэш соответствующих ключей сбросится – поэтому даже закешированные ранее отсутствующие записи или старые данные сразу станут неактуальными. (Важно: убедиться, что опция NOLOOP не используется при включении tracking, чтобы клиент-писатель тоже получил инвалидацию.)
Примечание: Мы сознательно делаем Lookup в два шага (отдельно inode и атрибут). Это немного увеличит число команд при холодном кэше, но Rueidis частично компенсирует это авто-пайплайном. Зато это решение проще и гарантирует корректную инвалидацию кэша. При желании, можно оптимизировать одним batched-запросом (см. Оптимизации).

Чтение списка каталога (Readdir) – здесь потенциально затрагивается много ключей, поэтому полностью кэшировать результат команды HScan нецелесообразно. Однако сами элементы каталога уже покрыты кэшированием точечных операций: каждая запись – это поле в хеше entry:{dir} и соответствующий inode ключ. При Readdir реализованном через несколько итераций HScan мы предлагаем не сохранять промежуточные результаты сканирования в кэше, а лишь использовать кэш при подгрузке атрибутов (fillAttr). В функции fillAttr вместо последовательных m.compat.Get для каждого inode можно использовать либо многокомандный кэш-запрос (через DoMultiCache Rueidis) сразу на группу ключей, либо вызывать cachedGet в цикле. Оба подхода приведут к тому, что атрибуты всех записей каталога закешируются локально. Например, можно подготовить массив команд []CacheableTTL при помощи rueidis.CT(builder.Get().Key(...).Cache(), ttl) для каждой inode и вызвать m.client.DoMultiCache(ctx, cmds...). После этого атрибуты будут в кэше на заданный срок. В итоге повторные чтения тех же каталогов или файлов будут обслуживаться быстрее.

Прочие операции чтения: аналогично исправляем получение extended attributes (GetXattr) – вместо m.compat.HGet используем cachedHGet для ключа xattr:{inode} (поле – имя атрибута). Также проверяем функции типа ReadLink (чтение символической ссылки, если хранится в Redis), ListXattr, StatFS (счётчики) и т.п. – все случаи, где выполняется GET/HGET/HMGET на Redis, должны идти через кэш. Везде, где ранее мог использоваться go-redis без кэша, переводим на Rueidis DoCache. Это гарантирует, что любой ключ, прочитанный клиентом, попадает в трекер Redis и будет своевременно сброшен при изменении.

Шаг 2.3: Обработка операций записи с учетом кэша. При выполнении модифицирующих операций (создание файла/директории, удаление, переименование, изменение атрибутов, записей xattr и т.д.) мы полагаемся на механизм invalidation, встроенный в Redis: после каждой команды SET, HSET, DEL и т.п. сервер отправит уведомления о затронутых ключах всем подписанным клиентам
github.com
. Нужно убедиться, что все такие операции действительно изменяют те же ключи, которые используются при чтении:

Например, Create/Mkdir: выполняется INCR счётчика inode, SET inode:{new} для атрибутов нового объекта и HSET entry:{parent} для добавления имени. В результате клиенты, у которых был закеширован entry:{parent} (или отсутствующее поле в нём), получат инвалидацию и при следующем Lookup сразу увидят новый файл. Атрибут нового inode никто до создания не читал, но на всякий случай создающий клиент после SET inode может сразу выполнить GET этого же ключа через DoCache – прайминг кэша после записи. Хотя Redis тут же пришлёт invalidation (в том числе себе самому), Rueidis умеет различать источник уведомления; при BCAST-режиме без NOLOOP даже локальный клиент сбросит кеш. Поэтому, чтобы не выполнять лишний сетевой запрос, можем не делать дополнительный GET сразу после SET. Вместо этого, возвращаем атрибут из памяти (у нас же есть структура Attr нового файла) обратно в Fuse, минуя повторное обращение к Redis. (Этот момент можно рассмотреть в оптимизациях: напрямую помещать известные значения в кэш Rueidis API не предоставляет, но повторный DoCache сразу после записи зачастую излишний.)

Rename: при перемещении/переименовании файла/директории изменяются как минимум два ключа entry: – старого родителя (удаление поля) и нового родителя (добавление). Оба ключа были потенциально закешированы; после HDEL и HSET сервер разошлёт уведомления по ним. Нужно убедиться, что мы используем одинаковые префиксы ключей для tracking – в JuiceFS обычно ключи именованы предсказуемо (inode:<id>, entry:<id> и т.д.). В настройках Rueidis клиентских опций ClientTrackingOptions должны быть указаны эти префиксы и BCAST
github.com
, чтобы рассылка охватывала все ключи метаданных. После внедрения CSC, все клиенты сразу сбросят кэш затронутых inode и каталогов и при последующих чтениях выполнят актуальные запросы.

Delete (Unlink/Rmdir): аналогично, выполняется HDEL entry:{parent} и (для файлов) добавление в список удалённых inode. Ключ каталога родителя будет инвалидирован. Если кто-то ранее запрашивал атрибут удаляемого файла (inode), он тоже должен сброситься – это произойдёт, поскольку при удалении JuiceFS обычно либо сразу помечает inode как удалённый (например, перемещая его в специальный список/множество delfiles), либо обновляет какой-то его флаг. Важно убедиться, что именно тот ключ, который читается при последующем GetAttr, будет изменён при удалении. Если нет – можно вручную сбросить кэш inode (например, через CLIENT CACHING). Но скорее всего, JuiceFS обновляет хотя бы TTL или метку inode при удалении, что вызовет инвалидацию.

Модификации атрибутов (SetAttr, Chmod, Truncate): эти операции выполняются командой SET inode:{id} или HSET inode:{id} поле. Раз мы читаем весь Attr по ключу inode:{id}, то любое изменение этого ключа должно сбросить кэш. После, например, Truncate (смена размера) клиент при следующем GetAttr заново получит свежие данные из Redis. При необходимости, можно праймить кэш: сразу после успешного SET нового атрибута сделать cachedGet того же ключа, чтобы повторный stat сразу пришёл из памяти. Однако, как сказано выше, сервер уже разошлёт инвалидацию — то есть кэш либо уже пуст, либо сбросится мгновенно, поэтому последующий GetAttr и так пойдёт в Redis. В целом, явный post-write priming имеет смысл только чтобы избежать лишнего скачка к Redis после каждой записи, но стоит оценить, не превысит ли суммарное число таких чтений выигрыш.

Шаг 2.4: Покрытие остальных путей и тестирование. Помимо основных функций, проверяем места, которые могли страдать от той же проблемы рассинхронизации:

Операции списка и сканирования метаданных: например, функция перечисления всех inode или chunk (типа ListChunks или внутренняя scanAllChunks). В коде rueidis.go уже есть реализация через SCAN/HSCAN без кэша (что нормально). Эти операции используются для консистентности данных (GC, инспекция), их лучше не кешировать. Они выполняются редко и должны всегда видеть актуальное состояние, поэтому оставляем их без изменений (прямой m.compat.Scan и т.д.).

Проверка поведении TTL: после внедрения CSC можно оставить большой TTL (час и более) для метаданных, поскольку теперь длительность жизни записи в кэше не повлияет на согласованность – Redis сам сбросит кэшированные данные раньше TTL при любом изменении ключа. Важно протестировать крайний случай: создание файла при очень большом TTL (например, 1 день) – файл должен сразу же появиться при ls на другом клиенте. И наоборот, часто читаемые, но не меняющиеся метаданные (например, корневой inode) должны выдерживать нагрузку чтений из кэша без повторных сетевых запросов.

После выполнения этих шагов, все клиенты JuiceFS на Rueidis будут работать в модели активного кэширования: чтения быстры, как из локальной БД, а изменения мгновенно распространяются, что соответствует поведению с локальной репликой, но без её поддержки
github.com
.

3. Изменения в файлах rueidis_lock.go и rueidis_bak.go

Блокировки (файл rueidis_lock.go): Механизм POSIX-lock в JuiceFS реализован через специальные ключи (например, хеши типа plock:{inode}) и использует транзакции Redis (WATCH/MULTI) для атомарности. В коде Rueidis это видно по использованию m.txn(ctx, func(tx rueidiscompat.Tx) {...}). Внутри транзакции выполняются команды чтения и записи через tx.HGet, tx.HGetAll, tx.HSet и т.д., которые не используют кэш (и не должны, так как транзакция должна работать с актуальным состоянием). Здесь изменений не требуется: для корректной работы блокировок мы оставляем прямые вызовы tx.HGet/HGetAll без Cache. Эти операции редки и локальны по времени, кэширование им не нужно. Главное – убедиться, что в коде вне транзакции нигде не кешируется состояние замков. На текущий момент запрашивать состояние блокировки напрямую клиенты не должны, поэтому дополнительного кода не требуется. Если же есть вспомогательные функции (например, считывающие m.lockedKey(sid) или содержимое хеша вне транзакции), их тоже не переводим на cached... – иначе возможна ситуация устаревших данных о локах. Таким образом, rueidis_lock.go правок не требует, кроме, возможно, комментария, что caching для замков отключён умышленно ради консистентности.

Резервное копирование (файл rueidis_bak.go): Операции бэкапа/восстановления метаданных (Dump/Load) в JuiceFS читают всю базу метаданных или её большие сегменты. В реализации для Rueidis эти методы используют m.compat напрямую (например, m.compat.Scan, HGetAll, SMembers, ZRange и т.п.) чтобы выгрузить все ключи и поля. Кэширование при бэкапе не нужно и даже вредно: мы однажды читаем каждый ключ, после чего эти данные не нужны клиенту, и засорять ими локальный кэш смысла нет. Кроме того, бэкап должен видеть самые актуальные данные (он обычно делается при отключённом обслуживании или на «холодном» standby-клиенте). Поэтому в rueidis_bak.go ничего менять не нужно – пусть остаются прямые обращения. Разве что стоит убедиться, что при бэкапе не вызываются случайно наши новые cachedGet/HGet: судя по коду, там явно разделена логика (функции dumpFormat/dumpCounters/... либо переопределены для Rueidis, либо унаследованы). Наши изменения коснутся только оперативных путей, а бэкап продолжит использовать m.redisMeta.dump* или m.compat. Таким образом, никаких багов в бэкапе из-за кэша не возникнет, и правки ограничиваются основным файлом rueidis.go.

(Итого, файлы rueidis_lock.go и rueidis_bak.go требуют лишь минимального ревью: убедиться, что они не используют неподходящие обёртки. Если вдруг в будущем решим кешировать некоторые статичные метаданные (например, формат ФС или счётчики) при бэкапе, это можно сделать, но сейчас приоритет – корректность.)

4. Оптимизации

В процессе решения основной проблемы были выявлены несколько потенциальных улучшений производительности и архитектуры, не требующих изменения внешнего API:

Объединение запросов для массовых операций: Теперь, когда есть удобные методы DoMultiCache, можно оптимизировать загрузку множества ключей. Например, в fillAttr при получении атрибутов сразу для сотен inode вместо цикла с последовательными cachedGet можно использовать один вызов DoMultiCache с пакетом команд или утилиту rueidis.MGetCache. Rueidis поддерживает групповой MGET с кэшированием, разбивая ключи по слотам и выполняя минимальное число запросов
github.com
. В случае standalone Redis все inode-ключи и так хранятся на одном сервере, поэтому возможен один MGET на всю партию. Это снизит накладные расходы автопайплайна и ускорит первоначальное «прогревание» кэша при массовых операциях (например, ls -l большого каталога).

Уточнение использования BCAST vs OPTIN: Мы включили широковещательный режим invalidation (BCAST) по префиксу метаданных, чтобы упростить согласованность между разными клиентами. В дальнейшем можно рассмотреть режим OPTIN (по умолчанию в Rueidis), при котором сервер будет слать уведомления только по ключам, действительно запрошенным данным клиентом
github.com
. Это может снизить трафик invalidation при наличии большого числа несвязанных ключей и множества клиентов: каждый получит только релевантные сбросы. В контексте JuiceFS, где у разных клиентов пересекаются общие каталоги, BCAST по общему префиксу тоже оправдан, но для масштабирования на тысячи клиентов OPTIN даст выгоду. Внедрить OPTIN несложно: убрать опцию "BCAST" из ClientTrackingOptions и всегда использовать DoCache для отметки нужных ключей. Уже выполненная замена чтений на cached... позволит этому режиму работать сразу.

Избежание лишних чтений сразу после записи: Как отмечалось, можно устранить некоторые ненужные сетевые операции, доверяя тому, что у нас уже есть актуальные данные. Например, после Create у нас в памяти есть новый Attr – мы возвращаем его напрямую, вместо вызова GetAttr. Аналогично, после Setattr/Truncate можно обновлять поля локальной структуры inode (если таковая есть в кэше FUSE) и не вызывать мгновенно GetAttr. В идеале, JuiceFS клиент мог бы иметь небольшое write-through кеширование на уровне приложения (например, хранить Attr недавно созданного или изменённого inode в структуре InodeCache на несколько секунд). Но даже без этого, наш подход с CSC уже достаточно хорош: первая же попытка чтения после изменения пойдёт к Redis (т.к. кэш сброшен), и разработчик может решить, что двойной обход (запись+чтение) для прайминга не окупается. Тем не менее, в самых горячих местах можно добавить опциональный прайминг: например, после операции, меняющей много полей (chmod/chown), выполнить один cachedGet – он будет обслужен из Redis (который, возможно, всё равно находится в той же локальной сети, что и раньше реплика). Поскольку таких операций относительно немного, это не ухудшит производительность заметно, а кэш обновится сразу.

Контроль размера и времени жизни кэша: По умолчанию Rueidis выделяет довольно большой объём под клиентский кэш (например, 128 МБ на соединение). Если известно, что метаданные файловой системы очень объёмные (десятки миллионов ключей), стоит настроить параметр CacheSizeEachConn в ClientOption. Также, TTL в 1 час – это компромисс между скоростью и памятью; возможно, оптимально его уменьшить (например, до 10-15 минут) – при активной работе разницы не будет (данные будут постоянно актуализироваться по уведомлениям), а память очистится быстрее для редко используемых объектов. Поскольку invalidation гарантирует согласованность, TTL можно рассматривать просто как «время неактивности», после которого данные выгружаются. Можно даже динамически менять TTL для разных типов данных: например, inode атрибуты кешировать час, а список каталогов – на меньший срок, если подозреваем, что они быстрее устареют. Эти тонкие настройки не меняют интерфейсов, но могут улучшить как потребление памяти, так и свежесть данных.

Тестирование и наблюдение: После внедрения стоит включить метрики Rueidis: методы IsCacheHit() и встроенные счётчики (rueidis_do_cache_hits/miss) помогут убедиться, что значительная часть операций идёт из кэша
github.com
. Если число промахов высоко, возможно, мы что-то не закешировали. Также, с включённым трекингом, можно периодически вызывать команду CLIENT TRACKINGINFO на Redis, чтобы увидеть, сколько ключей держит каждый клиент и сколько invalidation отправлено. Это поможет убедиться, что система ведёт себя как задумано: кэш ударов много, а рассылка invalidation предотвращает устаревшие данные.

В итоге, после выполнения данного плана, JuiceFS на Rueidis будет сочетать преимущества большой metadata cache TTL и согласованность данных. Производительность чтения метаданных останется высокой, сравнимой с работой через локальную реплику, а поведение станет корректным: новые файлы, изменения атрибутов и удаление объектов сразу видны всем клиентам без длительного ожидания. Это улучшение укрепит консистентность распределённой файловой системы без усложнения её развёртывания.